# Deep learning

## normalization
1.Feature Scaling (Min-Max Scaling):
Feature scaling, also known as Min-Max scaling, scales the input features to a specific range, usually [0, 1]. The formula for Min-Max scaling is:

This ensures that all feature values are within the [0, 1] range.
X = (X-Xmin) / (Xmax - Xmin)

2.Z-score Normalization (Standardization):
Z-score normalization, also known as standardization, transforms the input features to have a mean of 0 and a standard deviation of 1. The formula for Z-score normalization is:
X = X-μ /  σ
where μ is the mean of the feature, and σ is the standard deviation.

Normalization is beneficial for several reasons:

Faster Convergence: Normalized inputs can speed up the convergence of optimization algorithms during training.

Numerical Stability: Normalization helps prevent numerical instability, especially when using activation functions like sigmoid or tanh.

Equalizes Learning Rates: Normalization ensures that all features contribute more equally to the learning process.

Improved Generalization: It can help prevent the model from overfitting to specific features.

Normalization is commonly applied to the input features of neural networks, especially in cases where the features have different scales. 
In deep learning frameworks like TensorFlow and PyTorch, normalization layers (e.g., Batch Normalization) are also used within the neural 
network architecture to normalize the activations at different layers during training.

## 混淆矩阵
对于a类来说:
  TP: y_hat预测为a类, y为a类
  TN: y_hat预测为非a类, y为非a类
  FP: y_hat预测为a类, y为非a类
  FN: y_hat预测为非a类, y为a类
```python
for o, l in zip(out, label):
  # o:type_1, l:type_2
  if o == l:
    val_TP[o] += 1
  else:
    val_FP[o] += 1
    val_FN[l] += 1
```
Precision: TP / (TP + FP)
Accuracy: TP / (TP + TN + FP + FN)
Recall: TP / (TP + FN)
F1-score: 2 * Precision * Recall / (Precision + Recall) = TP / (TP + 0.5*(FP + FN))



