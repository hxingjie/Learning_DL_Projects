```python
X = torch.Tensor()  # 创建Tensor，默认float类型的Tensor

X = torch.tensor()  # 创建Tensor，使用函数创建
函数原型为def tensor(data: Any, dtype: Optional[_dtype]=None, device: Device=None, requires_grad: _bool=False) -> Tensor: ...

X.require_grad = True  # 直接设置属性
X.require_grad_(True)  # 通过方法设置属性

X.grad  # 获取梯度，也是Tensor
X.data  # 获取值
X.item()  # 获取值

optimizer.zero_grad()
outputs = net(images.to(device))
loss = loss_function(outputs, labels.to(device))
loss.backward()
optimizer.step()

# 卷积、池化
# N = (W - F + (P_row + P_col)) / S + 1
self.features = nn.Sequential(
    nn.ZeroPad2d((1, 2, 1, 2)),  # 左补一列, 右补两列, 上补一行, 下补两行
    nn.Conv2d(3, 48, kernel_size=11, stride=4, padding=2)
    # padding=2, 上下左右全部补2行(列)0
    # padding=(1,2), 上下各补一行0, 左右各补两列0
    nn.ReLU(inplace=True),
    nn.MaxPool2d(kernel_size=3, stride=2)
)

# 二分类
sigmoid + BCEloss
softmax + CrossEntropyLoss, 注意，pytorch中的CrossEntropyLoss已经集成了softmax

```
