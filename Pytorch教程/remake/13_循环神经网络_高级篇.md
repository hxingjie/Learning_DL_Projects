```python
import torch
import gzip
import csv
from torch.utils.data import Dataset
from torch.utils.data import DataLoader

import time


class NameDataset(Dataset):
    def __init__(self, is_train=True):
        filename = './dataset/names_train.csv.gz' if is_train else './dataset/names_test.csv.gz'
        with gzip.open(filename, 'rt') as data_file:
            reader = csv.reader(data_file)
            rows = list(reader)

        self.names = [row[0] for row in rows]
        self.len = len(self.names)

        self.countries = [row[1] for row in rows]
        self.countries_set_list = list(sorted(set(self.countries)))
        self.countries_dict = self.getCountryDict()
        self.country_cnt = len(self.countries_set_list)

    def __getitem__(self, idx) -> tuple:
        return self.names[idx], self.countries_dict[self.countries[idx]]

    def __len__(self) -> int:
        return self.len

    def getCountryDict(self) -> dict:
        countries_dict = {}
        for idx, country in enumerate(self.countries_set_list, 0):
            countries_dict[country] = idx

        return countries_dict

    def idx2country(self, idx) -> str:
        return self.countries_set_list[idx]

    def getCountriesCnt(self) -> int:
        return self.country_cnt


# Parameters
HIDDEN_SIZE = 100
BATCH_SIZE = 256
N_LAYER = 2
N_EPOCH = 100
N_CHARS = 128
USE_GPU = True
if USE_GPU:
    device = torch.device('cuda:0')

train_set = NameDataset(is_train=True)
train_loader = DataLoader(dataset=train_set, batch_size=BATCH_SIZE, shuffle=True)
test_set = NameDataset(is_train=False)
test_loader = DataLoader(dataset=test_set, batch_size=BATCH_SIZE, shuffle=False)

N_COUNTRY = train_set.getCountriesCnt()


def trans_device(tensor):
    if USE_GPU:
        tensor = tensor.to(device)
    return tensor


def name2list(name) -> tuple:
    arr = [ord(c) for c in name]  # ord(c): return c's ASCII
    return arr, len(arr)


def make_tensors(names, countries):
    sequences_and_lengths = [name2list(name) for name in names]
    sequences: list = [sl[0] for sl in sequences_and_lengths]
    lengths: torch.LongTensor = torch.LongTensor([sl[1] for sl in sequences_and_lengths])
    countries: torch.LongTensor = countries.long()  # 将tensor的数据类型转换为64位整数类型（torch.int64）

    sequences_pad = torch.zeros(len(sequences), lengths.max()).long()
    for idx, (sequence, length) in enumerate(zip(sequences, lengths), 0):
        sequences_pad[idx][:length] = torch.LongTensor(sequence)

    lengths, sorted_idx = lengths.sort(dim=0, descending=True)
    countries = countries[sorted_idx]
    sequences_pad = sequences_pad[sorted_idx]

    return trans_device(sequences_pad), lengths, trans_device(countries)


class RNNClassifier(torch.nn.Module):
    def __init__(self, input_size, hidden_size, output_size,
                 n_layers=1, bidirectional=True):
        super(RNNClassifier, self).__init__()
        self.hidden_size = hidden_size
        self.n_layers = n_layers
        self.n_directions = 2 if bidirectional else 1

        self.emb = torch.nn.Embedding(num_embeddings=input_size, embedding_dim=hidden_size)
        self.gru = torch.nn.GRU(input_size=hidden_size, hidden_size=hidden_size,
                                num_layers=n_layers, bidirectional=bidirectional)
        self.fc = torch.nn.Linear(in_features=hidden_size * self.n_directions,
                                  out_features=output_size)

    def _init_hidden(self, batch_size):
        hidden = torch.zeros(self.n_layers * self.n_directions,
                             batch_size, self.hidden_size)
        return trans_device(hidden)

    def forward(self, inputs, seq_lengths):
        inputs = inputs.T  # inputs: (B, Seq) -> (Seq, B)

        batch_size = inputs.shape[1]
        hidden = self._init_hidden(batch_size)

        emb = self.emb(inputs)  # emb: (Seq, B, hidden_size)
        # pack_padded_sequence's parameter lengths must be on cpu if it is tensor
        gru_inputs = torch.nn.utils.rnn.pack_padded_sequence(emb, lengths=seq_lengths)

        outputs, hidden = self.gru(gru_inputs, hidden)
        fc_input = hidden[-1] \
            if self.n_directions == 1 else torch.concat([hidden[-1], hidden[-2]], dim=1)

        fc_output = self.fc(fc_input)
        return fc_output


def trainModel():
    total_loss = 0
    for idx, (names, countries) in enumerate(train_loader, 1):
        sequences, lengths, targets = make_tensors(names, countries)
        outputs = classifier(sequences, lengths)
        loss = criterion(outputs, targets)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        total_loss += loss.item()
        if idx % 10 == 0:
            print(f'[Time_since{start}] Epoch {epoch} ', end=' ')
            print(f'[{idx * len(sequences)}]/{len(train_set)} ', end=' ')
            print(f'Loss = {total_loss / (idx * len(sequences))}', end=' ')
        return total_loss


def testModel():
    correct = 0
    total = len(test_set)
    print("Test")
    with torch.no_grad():
        for idx, (names, countries) in enumerate(test_loader, 1):
            sequences, lengths, targets = make_tensors(names, countries)
            outputs = classifier(sequences, lengths)
            pred = outputs.max(dim=1, keepdim=True)[1]
            correct += pred.eq(targets.view_as(pred)).sum().item()

        percent = '%.2f' % (100 * correct / total)
        print(f'Test: Accuracy {correct}/{total} {percent}%')
        return correct


if __name__ == '__main__':
    classifier = RNNClassifier(N_CHARS, HIDDEN_SIZE, N_COUNTRY, N_LAYER)
    if USE_GPU:
        classifier.to(device)

    criterion = torch.nn.CrossEntropyLoss()
    optimizer = torch.optim.Adam(classifier.parameters(), lr=0.001)

    start = time.time()
    print("Training for %d epochs " % N_EPOCH)
    acc_list = []
    for epoch in range(1, N_EPOCH+1):
        trainModel()
        acc = testModel()
        acc_list.append(acc)

```

