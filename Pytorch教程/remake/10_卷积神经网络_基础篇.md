```python
import torch
in_channels, out_channels = 5, 10
width, height = 100, 100
kernel_size = 3
batch_size = 1

input_val = torch.randn(batch_size, in_channels, width, height)

conv_layer = torch.nn.Conv2d(in_channels, out_channels, kernel_size=(batch_size, batch_size))
# padding = kernel_size / 2
# bias: 是否加偏置量

output_val = conv_layer(input_val)

print(input_val.shape)  # batch_size, in_channels, width, height
print(conv_layer.weight.shape)  # out_channels, in_channels, kernel_size_width, kernel_size_height
print(output_val.shape)  # batch_size, out_channels, width`, height`
```
---
## 卷积
```python
import torch

input_val = [3, 4, 6, 5, 7,
             2, 4, 6, 8, 2,
             1, 6, 7, 8, 4,
             9, 7, 4, 6, 2,
             3, 7, 5, 4, 1]
input_val = torch.tensor(input_val, dtype=torch.float32).reshape(1, 1, 5, 5)

conv_layer = torch.nn.Conv2d(1, 1, kernel_size=3, padding=1, stride=2, bias=False)  # in, out, kernel_size, padding, bias
kernel = torch.tensor([1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=torch.float32).reshape(1, 1, 3, 3)  # out, in, w, h
# kernel_size: 3 * 3, output_val小1圈 3/2
# kernel_size: 5 * 5, output_val小2圈 5/2
conv_layer.weight.data = kernel.data

output_val = conv_layer(input_val)

print(output_val)
```
---
## 下采样
```python
import torch
# 最大池化层
input_val = [3, 4, 6, 5,
             2, 4, 6, 8,
             1, 6, 7, 8,
             9, 7, 4, 6]
input_val = torch.tensor(input_val, dtype=torch.float32).reshape(1, 1, 4, 4)

maxpooling_layer = torch.nn.MaxPool2d(kernel_size=2, stride=2)

output_val = maxpooling_layer(input_val)

print(output_val)
```
---
## 卷积神经网络
```python
import torch
from torchvision import transforms
from torchvision import datasets
from torch.utils.data import DataLoader
import torch.nn.functional
import torch.optim

my_batch_size = 64
my_transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307, ), (0.3081, ))])
# mean=0.1307, standard deviation=0.3801

train_dataset = datasets.MNIST(root='./dataset/mnist/', train=True, download=False, transform=my_transform)
train_loader = DataLoader(train_dataset, shuffle=True, batch_size=my_batch_size)

test_dataset = datasets.MNIST(root='./dataset/mnist/', train=False, download=False, transform=my_transform)
test_loader = DataLoader(test_dataset, shuffle=False, batch_size=my_batch_size)


class Net(torch.nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.conv1 = torch.nn.Conv2d(1, 10, kernel_size=5)
        self.conv2 = torch.nn.Conv2d(10, 20, kernel_size=5)
        self.conv3= torch.nn.Conv2d(20, 10, kernel_size=3)
        self.pooling = torch.nn.MaxPool2d(2)

        self.fc1 = torch.nn.Linear(10, 40)
        self.fc2 = torch.nn.Linear(40, 20)
        self.fc3 = torch.nn.Linear(20, 10)

    def forward(self, x):
        # batch, 1, 28, 28
        batch_size = x.size(0)

        x = self.conv1(x)  # batch, 10, 24, 24
        x = torch.nn.functional.relu(x)
        x = self.pooling(x)  # batch, 10, 12, 12

        x = self.conv2(x)  # batch, 20, 8, 8
        x = torch.nn.functional.relu(x)
        x = self.pooling(x)  # batch, 20, 4, 4

        x = self.conv3(x)  # batch, 10, 2, 2
        x = torch.nn.functional.relu(x)
        x = self.pooling(x)  # batch, 10, 1, 1

        x = x.reshape(batch_size, -1)  # batch, 10
        # -1 表示自动推断

        x = self.fc1(x)  # batch, 40
        x = torch.nn.functional.relu(x)
        x = self.fc2(x)  # batch, 20
        x = torch.nn.functional.relu(x)
        x = self.fc3(x)  # batch, 10

        return x


model = Net()
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")  # gpu
model.to(device)  # gpu

criterion = torch.nn.CrossEntropyLoss()
optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.5)


def train(epoch):
    sum_loss = 0.0
    batch_idx = 0
    for data in train_loader:
        inputs, target = data
        inputs, target = inputs.to(device), target.to(device)  # gpu

        outputs = model(inputs)
        loss = criterion(outputs, target)

        optimizer.zero_grad()
        loss.backward()

        optimizer.step()

        sum_loss += loss.item()
        batch_idx += 1
        if batch_idx % 300 == 299:
            print('[%d, %5d] loss: %.3f' % (epoch, batch_idx, sum_loss / 300))
            sum_loss = 0.0


def test():
    correct = 0
    total = 0
    with torch.no_grad():
        for data in test_loader:
            images, labels = data
            images, labels = images.to(device), labels.to(device)  # gpu

            outputs = model(images)

            val, index = torch.max(outputs.data, dim=1)  # 对第1维求最大值，val是值，pred其实是下标

            total += labels.size(0)

            result = index == labels
            correct += result.sum().item()
    print('%d%%' % (100 * correct / total))


if __name__ == '__main__':
    for epoch in range(1):
        train(epoch)
        test()

```
